\chapter{Soluzione vincente}
La soluzione vincente si basa su grafi metrici.

Più precisamente crea una struttura di indicizzazione a grafo ibrida, questa struttura 
è composta da un insieme di grafi HNSW, ciascuno contenente un insieme di vettori 
sufficientemente grande con metadati comuni. Ad esempio: crea un HNSW per ogni singola 
categoria sufficientemente grande e un HNSW per intervallo di timestamp.

\section{HNSW}
Hierarchical Navigable Small World Graphs (HNSW) è una struttura dati basata su 
grafi metrici a livelli che permette di navigare
lo spazio dei vettori attravero un cammino composto da archi che collegano due nodi 
vicini. Ogni livello è una 
rappresentazione dello spazio gentrico tramite un grafo metrico. Nel grafo ogni nodo rappresenta 
un vettore e si hanno degli archi diretti che rappresentano la relazione di vicinanza,
ovvero, dato un nodo ogni suo arco uscente porta ai $M$ nodi più vicini a quel nodo. La relazione 
di vicinanza è diretta perché nel caso dell'arco $(a,b)$, non è detto che il nodo 
$b$ abbia nei $M$ nodi più vicini il nodo $a$.

Ogni livello rappresenta il livello di approssimazione dello spazio geometrico, 
più il livello è alto allora maggiore è l'approssimazione, più il livello è basso 
e più precisa sarà la rappresentazione dello spazio. Dato un livello $l$, il livello 
successivo $l-1$ è un grafo composto da almeno gli stessi nodi del grafo sul livello 
precedente con l'aggiunta di nuovi nodi collegati ai nodi già presenti secondo 
la relazione di vicinanza (nel nostro caso si basa sulla distanza euclidea). Vedi 
figura \ref{fig:hnsw} 

Questo permette di navigare lo spazio velocemente nel livello più alto non passando 
per tutti i punti perché ogni nodo di un livello rappresenta una \textbf{Voronoi Cell} dello spazio grande 
quanto la specificità del livello stesso. Nodi presenti in un grafo di livello 
superiore rappresenteranno delle Voronoi Cell più grandi rispetto ai nodi 
presenti in un grafo di un livello inferiore.

\begin{figure}
    \centering
    \includegraphics*[width=0.5\textwidth]{img/hnsw.png}
    \caption{Esempio grafico di un HNSW a $3$ livelli. Il percorso con le frecce blu corrisponde 
    al percorso compiuto per cercare il nodo giallo nella struttura. 
    Il livello $0$ rappresenta l'intero spazio metrico, mentre i livelli superiori sono delle 
    approssimazioni. \cite{hnsw}}
    \label{fig:hnsw}
\end{figure}

\subsection{Creazione di HNSW}

Dato un insieme di vettori $V$, la costruzione di un generico HNSW  è un processo 
iterativo, in cui si inserisce ciascun vettore uno alla volta. L'inserimento di 
un vettore $q\in V$ nella struttura si articola nelle seguenti fasi:
\begin{itemize}
    \item \textbf{inizializzazione}: si seglie un nodo $ep$ di entry point nel livello massimo
    e si seleziona il livello $l$ a partire dal quale si inserice il vettore $q$.
    Più precisamente $l = \lfloor-\ln (U[0,1])\cdot m_l\rfloor$  e si inserirà $q$ 
    in tutti i livelli da $l$ a $0$. $m_l$ è la costante di normalizzazione che 
    limita $l\in [0, L_{max}]$, dove $L_{max}$ è il livello massimo della struttura.
    \begin{figure}
        \centering
        \includegraphics*[width=0.5\textwidth]{img/exp_decay_distr.png}
        \caption{Andamento di una funzione di densità di decadimento esponenziale. 
        \cite{exp_decay}}
        \label{fig:exp_decay}
    \end{figure}
    \item \textbf{prima fase}: a partire dal livello $L_{max}$, si naviga il grafo 
    a partire dal nodo $ep$ fino a quando non si trova il nodo $w$ più vicino a
    $q$ (algoritmo greedy), successivamente si effettua la stessa operazione 
    su livello $L_{max} - 1$, utilizzando come nodo di entry point il nodo $w$,
    si itera il procedimento fino a quando non si trova l'entry point $w$ del livello $l$.
    \item \textbf{seconda fase}: a partire dal livello $l$ si cercano i $W$ nodi 
    più vicini a $q$ a partire dall'ultimo entry point trovato al termine della fase precedente ($|W| = efConstruction$),
    sempre col medesimo algoritmo greedy accennato nella prima fase.
    Successivamente si usa un'euristica che calcola dall'insieme dei $W$ nodi
    l'insieme $N$ degli $M$ nodi più vicini a $q$ sempre sullo stesso layer $l$. 
    Si creano degli edge bidirezionali tra i nodi $N$ e $q$. Successivamente si 
    sistemano i collegamenti tra i vicini in modo che ogni nodo rispetti il vincolo 
    di avere al massimo $M_{max}$ collegamenti, questo viene implementato rimuovendo 
    degli edge tra due nodi nel vicinato di $q$. Infine si reitera il procedimento 
    dal livello $l$ al livello $0$.
\end{itemize}

\begin{nota}
    La scelta del livello dal quale inserire un nuovo vettore viene effettuata 
    in modo randomico secondo una \textbf{distribuzione di decadimento esponenziale}. 
    In sostanza si ha maggior probabilità di estrarre un livello $l$ vicino a $0$ 
    piuttosto che un livello più vicino a $L_{max}$. (vedi figura \cite{exp_decay})
\end{nota}

\begin{nota}
    L'algoritmo greedy accennato precedentemente permette di navigare il grafo 
    ad un particolare livello a partire da un nodo di entrypoint $ep$ e trovare gli $ef$ 
    nodi candidati vicini al vettore $q$. 
    La ricerca avviene nel seguente modo:
    \begin{itemize}
        \item si tiene traccia dei nodi già visitati, dei nodi vicini a $q$. Inizialimente 
        entrambi gli insiemi sono vuoti.
        \item si parte dal nodo $ep$, si aggiunge $ep$ all'insieme dei nodi vicini a $q$,
        si guardano tutti i vicini e si calcola la distanza da ciascun vicino a $q$,
        si sceglie il vicino a distanza minima e ci si sposta in quel nodo, si 
        aggiunge $ep$ all'insieme dei nodi visitati.
        \item si reitera il punto precedente, evitando di visitare nodi già visitati e
        fino a quando non si può più andare avanti, oppure si continua fino a quando 
        non si arriva ad una iterazione massima. Se l'insieme dei vicini a $q$
        superla $ef$ allora vengono rimossi i nodi più lontani da $q$.
    \end{itemize} 
\end{nota}

Per la costruzione della struttura si richiedono i seguenti parametri:
\begin{itemize}
    \item $M_{max}$: numero massimo di edge uscenti da ogni nodo per un livello, questo 
    potrebbe coincidere con $M$.
    \item $L_{max}$: livello massimo della struttura.
    \item $efConstruction$: specifica il numero dei nodi candidati ad essere i vicini 
    del vettore $q$. Questo parametro permette di variare la recall e il tempo di 
    costruzione della struttura.
\end{itemize} 



\subsection{Ricerca nella struttura}
La ricerca di una query $q$ in un generico HNSW richiede i seguenti parametri:
\begin{itemize}
    \item $K$: numero di vicini alla query $q$ da cercare nella struttura
    \item $ef$: numero di candidati vicini da considerare quando si naviga sui singoli 
    grafi di ciascun livello.
\end{itemize}

La ricerca si articola nei seguenti passi:
\begin{itemize}
    \item si sceglie randomicamente un entry point $ep$ nel livello massimo
    \item nel livello massimo si cercano gli $ef$ migliori vicini alla query 
    $q$ mediante l'algoritmo greedy di graph traversal usato in fase di costruzione
    \item si estrae dai migliori $ef$ vicini alla query il più vicino e lo si usa 
    come $ep$ per il grafo al livello inferiore.
    \item si reitera il procedimento fino a quando non si ottengono gli $ef$ vicini 
    alla query al livello $0$. In questo caso si ritornano i migliori $K$ vicini.
\end{itemize}  

\section{Creazione della struttura di indicizzazione}
Per prima cosa vene letto tutto il dataset e viene salvato
all'interno di $3$ array:
\begin{itemize}
    \item \texttt{base\_vecs}: array contenente i coefficienti di tutti i vettori del dataset
    di dimensione $10^7\cdot 100$.
    \item \texttt{base\_labels}: array contente le labels di tutti i vettori, ovvero un 
    vettore di $10^7$ elementi.
\end{itemize} 

\begin{nota}
    La posizione $i$ indica  l'informazione dell'$i$-esimo vettore nel dataset.
\end{nota}

Successivamente si creano degli array di appoggio che specificano l'ordinamento 
del dataset rispetto a diversi criteri di ordinamento:
\begin{itemize}
    \item \texttt{sorted\_base\_ids}: vettore contenente gli indici dei vettori ordinati 
    per categoria crescente e successivamente per timestamps. Quindi per ogni $i\le j\in \mathbb{N}$,
    allora il numero della categoria associato al vettore $sorted\_base\_ids[i]$
    è minore rispetto al numero della categoria del vettore $sorted\_base\_ids[j]$,
    in caso di categorie uguali allora si ordina per timestamps.
    \item \texttt{sorted\_base\_ids\_by\_time}: vettore contenente gli indici dei vettori ordinati 
    per timestamps spostando i spostando tutti i vettori della categoria più grande all'inizio 
    del vettore. In questo modo all'inizio del vettore si avranno solo vettori 
    ordinati per timestamps della categoria più grande, successivamente si hanno 
    tutti gli altri vettori ordinati per timestamp. 
    \item \texttt{sorted\_base\_ids\_by\_full\_time}: vettore contenente gli indici dei vettori ordinati 
    per timestamps.
\end{itemize}

\begin{nota}
    L'ordinamento dei vettori della categoria più grande in sorted\_base\_ids\_by\_time 
    viene replicato dal vettore sorted\_base\_ids perché in caso ci fossero vettori 
    della categoria più grande con lo stesso timestap allora questi possono avere 
    un ordinamento diverso tra i due indici.
\end{nota}

In aggiunta viene creata una $c\_map$, ovvero una mappa delle categorie ovvero 
un array di coppie ordinate:
\begin{equation}
    category\_map[c] = \langle i, dimensione\_categoria\_c\rangle
\end{equation} 
dove $i = sorted\_base\_ids[v]$ con $v$ ultimo vettore della categoria $c$, 
mentre $dimensione\_categoria\_c$ specifica il numero di vettori presenti nella 
categoria $c$. Da notare che la coppia viene aggiunta all'array solo quando $c$ 
ha almeno $450000$ vettori.

Successivamente si costruisce una $t\_map$, ovvero una mappa dei timestamp ovvero 
un array di coppie ordinate:
\begin{equation}
    t\_map[i] = \langle l, dimensione\_intervallo\rangle
\end{equation} 
dove:
\begin{itemize}
    \item $i\in \{0,\dots,9\}$ specifica l'intervallo di timestamp $[0.1*i, 0.1*(i+1)]$
    \item $l= timestamps\_by\_full\_time[v]$, $v$ è il primo vettore con il timestamp nell'intervallo cercato
    \item $dimensione\_intervallo$ il numero di vettori nell'intervallo
\end{itemize}

In seguito viene letto il query set e viene salvato all'interno di due array:
\begin{itemize}
    \item \texttt{query\_vecs}: array contente le componenti dei vettori di query ovvero 
    $10^6 \cdot 100$.
    \item \texttt{query\_metas}: array contente i metadati delle query, ovvero la tipologia,
    la categoria e l'intervallo di timestamp.
    $10^6 \cdot 4$.
\end{itemize}

Successivamente si creano degli array di appoggio:
\begin{itemize}
    \item \texttt{sorted\_ids}: vettore degli indici dei delle query ordinate per tipo, 
    categoria, lower bound e upper bound dell'intervallo di timestamp
    \item \texttt{category\_query}: mappa che associa per ogni categoria un vettore di 
    indici di query che afferiscono a quella categoria.
\end{itemize}

Da notare che la $c\_map$, $category\_query$ e $t\_map$ sono tutte mappe implementate 
come \texttt{unordered\_map}.

Dopo la lettura ordinata del dataset e del queryset, è stata creata la struttura 
di indicizzazione sul dataset. Più precisamente è stato creato un HSNW per i vettori 
afferenti alla categoria più grande, uno per i vettori afferenti a categorie che 
contengono almento $500000$ vettori e uno per ogni intervallo di timestamp della 
$t\_map$. 

In aggiunta, vengono calcolate tutte le codifiche della quantizzazione scalare 
di tutti i vettori del dataset. Inoltre, si creano due vettori di ordinamento 
che specificano l'ordinamento delle condifiche:
\begin{itemize}
    \item \texttt{sorted\_both}: in posizione $i$ contiene i metadati dell'$i$-esima
    codifica dei vettori del dataset secondo l'ordinamento per categoria e successivamente
    per timestamp.
    \item \texttt{sorted\_timestamp}: in posizione $i$ contiene i metadati dell'$i$-esima
    codifica dei vettori del dataset secondo l'ordinamento per timestamp.
\end{itemize}

%La struttura di indicizzazione definita dagli autori è composta da:
%\begin{itemize}
%    \item \texttt{base\_vecs}
%    \item \texttt{base\_labels}
%    \item \texttt{sorted\_base\_ids}
%    \item \texttt{sorted\_base\_ids\_by\_time} 
%    \item \texttt{sorted\_base\_ids\_by\_full\_time}
%    \item \texttt{sorted\_both}
%    \item \texttt{sorted\_timestamp}
%    \item \texttt{c\_map}
%    \item \texttt{t\_map}
%    \item un grafo HNSW per la categoria più grande.
%    \item un grafo HNSW per ogni categoria più grande di $500000$ vettori.
%    \item un grafo HNSW per ogni intervallo di timestamp, in totale $10$.
%\end{itemize}

\subsection{Implementazione di HNSW}

\paragraph{Configurazione}

Vengono costruiti un grafo HNSW per la categoria $C_b$ pi\`u grande, uno per ogni categoria $C_i \ne C_b$ con $|C_i| > 500000$ e infine uno per ogni intervallo di timestamp nella $t_map$.
I parametri di costruzione degli HNSW specificati dagli autori sono $M = 28$ e $efConstruction = 200$.
In generale il numero di massimo di layer ottenibili nell'HNSW scala come $\frac 1 {log(M)}$, ovvero con una probabilit\`a molto piccola (si richiama la sezione d'introduzione su HNSW: la frazione \`e moltiplicata per una variabile uniforme $x \; : \; [0,1]$) verr\`a estratto un layer superiore allo zero. Con questa configurazione di parametri ci si pu\`o aspettare asintoticamente un numero molto basso di layer.

\paragraph{Il Wrapper}

Gli autori della soluzione hanno sfruttato l'implementazione di HNSW della libreria \href{https://github.com/zilliztech/pyglass/tree/master}{pyglass} creando un wrapper per le strutture definite dalla libreria.
Questo wrapper applica una quantizzazione scalare simmetrica a $8$ bit ai dati in ingresso (in modo tale da ridurre la dimensione 
dei vettori ad un totale di $112 B$ rispetto ai $400 B$ iniziali) per sfruttare le operazioni vettoriali del processore, velocizzando il calcolo delle distanze.
Quindi aggiunge i punti sequenzialmente ma sfruttando la parallelizzazione: questo grazie alla presenza e uso di lock all'interno dell'implementazione dell'HNSW di \textit{pyglass}.
Il primo punto inserito \`e per\`o escluso dalla parallelizzazione, altrimenti eseguendo in parallelo verrebberp aggiungiunti fino a 32 nodi come entry point.
Dopo aver inserito tutti i dati, il wrapper costruisce un grafo che copia sostanzialmente il Layer 0 dell'HNSW, tracciando oltre agli indici dei nodi puntati dagli archi il timestamp di ogni nodo bersaglio.
Questo grafo contiene anche un \textbf{Initializer} utilizzato in fase di ricerca per inizializzare il pool di candidati nodi vicini.

\paragraph{La Libreria}

L'algoritmo di inserimento dei punti implementato dalla libreria ricalca essenzialmente gli algoritmi presentati nella sezione precedente e presentati dal paper ufficiale di HNSW, tuttavia applica una serie di ottimizzazioni che lo rendono pi\`u veloce di una implementazione naive.

Per cominciare, la fase iniziale di ricerca per l'entry point pi\`u vicino viene effettuata tramite una variante della ricerca su Layer che non tiene traccia di tutti i nodi identificati e che sono candidati ma solo del pi\`u vicino, quindi ogni iterazione all'interno di un livello tiene conto solo dell'euristicamente "migliore" vicino.
Questo ha l'effetto di velocizzare quella fase a discapito della previsione, ragion per cui nella fase successiva si tiene traccia di una coda di candidati per evitare di perdere informazioni.

Nella funzione di ricerca su layer vera e propria invece tra gli accorgimenti troviamo delle istruzioni Streaming SIMD Extension che permettono all'hardware di precaricare le pagine con i dati da usare nella Cache, il che velocizza i tempi di accesso in maniera sensibile.
Inoltre nella fase di aggiornamento dei collegamenti in un layer si ritarda l'inserimento dell'arco inverso dal vicino i-esimo al punto che si sta inserendo e si sfrutta l'euristica di selezione di migliori M vicini dalla lista dei candidati (in questo caso il vicinato del vicino e il punto in inserimento).

Infine il fatto di inserire parallelamente i punti permette ad ogni sottoricerca di considerare meno punti e questo snellisce di poco l'aggiunta di nuovi nodi al grafo.

\section{Ricerca nella struttura di indicizzazione}

La ricerca nella struttura a grafo ibrida ha diverse strategie in base alle singole 
query. In generale si effettua una ricerca sfruttando le codifiche dei vettori, 
in questo caso si estrae un insieme di vettori maggiore rispetto a quelli richiesti 
dalla challenge, in generale almeno $140$. Poi in un secondo momento si raffina 
il risultato andando ad estrare dai questi $140$ i migliori $100$ usando la loro 
rappresentazione nello spazio originale non quantizzato. 

La prima operazione della ricerca si basa sull'ordinamento di tutte le query 
secondo due criteri differenti:
\begin{itemize}
    \item il primo per le label e poi per timestamp
    \item il secondo per timestamp
\end{itemize} 
Successivamente per ogni query viene calcolata la sua codifica usando la quantizzazione 
scalare sui coefficienti del vettore di query.

In seguito, viene calcolata la selettività di ciascuna query, per selettività
si intende il numero totale di vettori candidati ad essere potenziali vicini alla 
query rispetto i parametri di ricerca sui metadati. 
La selettività viene calcolata in tempo costante confrontando solo i metadati in 
base alla tipologia di query, sfruttando la $c\_map$ e la $t\_map$. 

La selettività di una query determina, insieme ai metadati della query, la strategia 
da adottare in fase di ricerca.

La \textbf{prima strategia} di ricerca è nel caso in cui la query è di tipo $1,2,3$ 
e la sua selettività è inferiore rispettivamente a $500000$, per i primi due, e $800000$, 
per l'ultimo tipo di query. In questo caso 
si effettua una ricerca esaustiva sfruttando le codifiche dei vettori candidati 
e gli array di ordinamento delle codifiche (\texttt{sorted\_both} e \texttt{sorted\_timestamp}).
Durante la ricerca si estraggono $140$ vettori vicini al vettore di query nello 
spazio delle condifiche e infine, di questi vettori, si scelgono $100$ vettori 
più vicini alla query nello spazio originale. 

La \textbf{seconda strategia} di ricerca è per le query di tipo $1$ che ha una selettività 
maggiore di $500000$ vettori. In questo caso la ricerca si effettua nel grafo 
HNSW della stessa categoria del metadato della query. I parametri di ricerca nell'HNSW
sono i seguenti 
$$K = 140 \qquad M= \lceil1800+(700)/(maxc\_size - minc\_size)dim\_cat\rceil$$
Dove:
\begin{itemize}
    \item $maxc\_size$: è la dimensione della categoria più grande presente 
    nel dataset.
    \item $minc\_size$: dimensione della categoria più piccola presente.
    nel dataset
    \item $dim\_cat$: dimensione della categoria in cui afferisce la query.
\end{itemize}
Dal risultato estratto dal grafo si effettua un raffinamento andando ad estrarre 
i $100$ migliori vettori nello spazio originale.

La \textbf{terza strategia} di ricerca è per le query di tipo $3$ che ha una selettività 
maggiore di $800000$ vettori. In questo caso la strategia è identica alla seconda 
ma con i seguenti parametri di ricerca nell'HNSW.(\textbf{RICERCA FILTRATA (OVVIAMENTE NON SI CAPISCE NULLA)}) 
$$K = 150 \qquad M= \lceil1800+(1000)/(maxc\_size - minc\_size)dim\_cat\rceil$$

La \textbf{quarta strategia} di ricerca è per le query di tipo $0$. In questo caso 
si effettua una ricerca parallela nei $10$ HNSW dei sotto-intervalli di timestamp 
usando per ognuna i seguenti parametri:
$$M=450 \qquad K=150$$
In questo modo si ottengono $150\cdot 10$ vicini alla query nello spazio delle 
codifiche, successivamente si raffinano i risultati estraendo i migliori $100$ 
vettori rispetto allo spazio originale.

La \textbf{quinta strategia} di ricerca è per la query di tipo $2$ che ha una 
superiore a $500000$. In questa strategia si sfruttano gli HNSW creati per i $10$
sottointervalli di timestamp, più precisamente si controlla la copertura dell'intervallo 
$[l,r]$ della query rispetto ai sottointervalli $[l',r']$ e se $[l,r]\cap [l',r']$ 
allora si ricerca nell'HNSW del sottointervallo $[l',r']$. 
In particolare in base alla selettività della query variano i parametri di ricerca:
\begin{itemize}
    \item se la query ha una selettività inferiore a $2000000$ di vettori, allora 
    per un generico sottointervallo $[l',r']$ abbiamo le seguenti casistiche:
    \begin{itemize}
        \item $[l,r]\cap [l',r'] = \emptyset$: si ignora il sottointervallo perché 
        l'HNSW contiene solo vettori in $[l',r']$ che sono fuori dall'intervallo 
        di query.
        \item $[l,r] = [l',r'] \lor  [l',r'] \subseteq [l,r]$: tutti i vettori 
        del sottointervallo solo potenziali candidati, quindi si effettua una ricerca nell'HNSW
        utilizzando parametri $M=780$ e $K=150$.
        \item $[l',r'] \cap [l,r] \ne \emptyset, [l',r'] \not \subseteq [l,r]$: 
        solo una parte dei vettori sono dei potenziali candidati, quindi in base 
        all'ampiezza di $[l',r'] \cap [l,r]$ si hanno diverse strategie:
        \begin{itemize}
            \item se l'intersezione è inferiore al $50\%$ di $[l',r']$ allora si effettua una 
            ricerca esaustiva delle $150$ codifiche più vicine alla codifica del 
            vettore di query. Per fare ciò si sfrutta la $t_map$ e 
            l'ordinamento delle codifiche rispetto il timestamp.
            \item se l'intersezione è superiore al $50\%$ di $[l',r']$ allora si effettua una 
            ricerca nell'HNSW del sottointervallo $[l',r']$ con i parametri $M=1180$ e $K=150$.
        \end{itemize}
    \end{itemize}
    \item se la query ha una selettività superiore a $2000000$ di vettori, allora 
    per un generico sottointervallo $[l',r']$ abbiamo le stesse casistiche 
    del caso con selettività inferiore a $2000000$, con la differenza che cambia
    la soglia per la ricerca esaustiva e i parametri della ricerca nel grafo. Più 
    precisamente:
    \begin{itemize}
        \item se $[l',r'] \not \subseteq [l,r]$ allora si ignora il sottointervallo.
        \item se $[l,r] = [l',r'] \lor  [l',r'] \subseteq [l,r]$:
        \begin{itemize}
            \item se la query seleziona fino a $3000000$ vettori allora ricerca 
            nell'HNSW di $[l',r']$ con $K=150$ e $M=1180$
            \item se la query seleziona da $3000000$ a $6000000$ vettori allora ricerca 
            nell'HNSW di $[l',r']$ con $K=150$ e $M=780$
            \item se la query seleziona sopra $6000000$ vettori allora ricerca 
            nell'HNSW di $[l',r']$ con $K=150$ e $M=680$
        \end{itemize}
        \item se $[l',r'] \cap [l,r] \ne \emptyset, [l',r'] \not \subseteq [l,r]$:
        \begin{itemize}
            \item se la query seleziona fino a $3000000$ vettori allora ricerca 
            nell'HNSW di $[l',r']$ con $K=150$ e $M=780$
            \item se la query seleziona da $3000000$ a $6000000$ vettori allora ricerca 
            nell'HNSW di $[l',r']$ con $K=150$ e $M=680$
            \item se la query seleziona sopra $6000000$ vettori allora ricerca 
            nell'HNSW di $[l',r']$ con $K=150$ e $M=480$
        \end{itemize}
    \end{itemize}
\end{itemize}

Quindi alla fine si producono un totale di $150$ vettori per ogni sottointervallo
$[l',r']$ con intersezione non nulla con l'intervallo di query e, in conclusione, 
si raffina il risultato estraendo i migliori $100$ vettori più vicini alla query 
nello spazio originale.   

\subsection{Considerazioni sulla ricerca}

Dal momento che sono state definite diverse strategie di ricerca, questo rende 
complicato la sua analisi di complessità di ricerca di una query.

L'osservazione più naturale che nasce dopo aver visto per la prima volta la ricerca,
è che la quantizzazione è l'elemento chiave. Infatti, calcolare la distanza sugli 
elementi quantizzazioni risulta nettamente più efficiente rispetto a calcolare la 
distanza sugli elementi, questo permette quindi agli autori di effettuare ricerche 
esaustive su numerosi elementi, cosa che noi non potevamo permetterci di fare.

Inoltre, la ricerca consiste alla fine nell'effettuare una ricerca esausiva 
oppure nell'effettuare una ricerca nell'HNSW. In generale la ricerca esaustiva è 
lineare ma limitata a massimo $1000000$ di vettori (nel caso della ricerca di una 
query di tipo 2), difficile che possa capitare dal momento che sarebbero tutti 
all'interno di un sottointervallo. Al contrario la ricerca nell'HNSW si effettua 
in tempo logaritmico e questo permette di velocizzare le ricerche per query ad 
alta selettività. 


% si articola nelle seguenti fasi:
% \begin{itemize}
%     \item codifica tutti i vettori, definisce due ordinamenti delle codifiche:
%     \begin{itemize}
%         \item il primo che ordina per label e timestamp
%         \item il secondo che ordina per timestamp
%     \end{itemize}
%     \item calcola le codifiche di tutte le query per calcolare le distanze in modo 
%     più efficiente.
%     \item si calcola la selettività di ciascuna query del tipo $1,2,3$, ovvero 
%     si calcolano il numero totale di candidati che rispettano i criteri di filtraggio 
%     per ciascuna query. Questo è stato implementato in tempo costante mediante la 
%     $t\_map$, la $c\_map$ e i due ordinamenti dei vettori effettuati in precedenza rispetto 
%     la categoria e rispetto il timestamp. 
%     \item se la query di tipo $1,2$ seleziona un numero di vettori 
%     inferiore a $500000$ o se la query di tipo $3$ seleziona un numero di vettori 
%     inferiore a $800000$ allora si effettua una ricerca esaustiva usando le codifiche 
%     dei vettori e la codifica della query. In questo modo si estraggono le $140$
%     migliori codifiche più vicine alla codifica della query (usando la distanza 
%     calcolata sulle codifiche) e si selezionano, tra queste $140$, i migliori $100$
%     vettori più vicini alla query usando la loro rappresentazione nativa e quindi 
%     calcolando le distanze sulle versioni di dimensione $100$.
%     \item se la query di tipo $1$ seleziona un numero di vettori 
%     superiore a $500000$ o se la query di tipo $3$ seleziona un numero di vettori 
%     superiore a $800000$ allora si effettua una ricerca nel grafo metrico HNSW 
%     dedicato alla categoria della query. I parametri della ricerca nell'HNSW sono
%     rispettivamente:
%     \begin{itemize}
%         \item per la query di tipo $1$: cerca un totale di $K=140$ vettori considerando 
%         un totale $M= \lceil1800+(700)/(maxc\_size - minc\_size)dim\_cat\rceil$ 
%         nodi vicini durante la ricerca. Dove $maxc\_size$ è la dimensione della categoria 
%         più grande presente 
%         nel dataset, $minc\_size$ dimensione della categoria più piccola presente 
%         nel dataset e $dim\_cat$ dimensione della categoria in cui afferisce la query.
%         \item per la query di tipo $3$: cerca un totale di $K=140$ vettori considerando 
%         un totale $M= \lceil1800+(1000)/(maxc\_size - minc\_size)dim\_cat\rceil$ 
%         nodi vicini durante la ricerca (le variabili per $M$ sono le stesse introdotte 
%         al punto precedente).  (\textbf{RICERCA FILTRATA (OVVIAMENTE NON SI CAPISCE NULLA)})
%     \end{itemize} 
%     Infine una volta ottenuti i migliori $140$ vettori si estraggono i migliori $100$
%     più vicini usando la distanza euclidea sulla rappresentazione dello spazio 
%     originale.
%     \item se la query è di tipo $0$ allora si effettua una ricerca parallela nei 
%     $10$ HNSW dei sotto-intervalli di timestamp usando per ognuna i seguenti parametri:
%     \begin{itemize}
%         \item $M=450$
%         \item $K=140$
%     \end{itemize} 
%     Infine si ottengono i migliori $100$ vettori nello spazio originale che sono 
%     a distanza minima rispetto ai totali $140\cdot 10$ vettori candidati.
%     \item se la query è di tipo $2$ e seleziona un numero di vettori 
%     superiore a $500000$ allora si applica una stategia di ricerca in base alla 
%     copertura dell'intervallo di timpestamp della query $[l,r]$ rispetto ai $10$ sottointervalli
%     di timestamp indicizzati. Più precisamente:
%     \begin{itemize}
%         \item se la query seleziona meno di $2000000$ di vettori allora per un 
%         generico sottointervallo $[l',r']$ di timestamp indicizzato:
%         \begin{itemize}
%             \item se $[l,r]$ non copre $[l',r']$ allora si ignora il sottointervallo.
%             \item se $[l,r] = [l',r'] \lor  [l',r'] \subseteq [l,r]$ allora 
%             tutti i vettori del sottointervallo solo potenziali candidati, 
%             quindi si effettua una ricerca nell'HNSW
%             utilizzando parametri $M=780$ e $K=150$.
%             Infine dei $150$ vicini alla query estratti, si scelgono i migliori $100$ 
%             nello spazio originale.
%             \item se $[l',r'] \cap [l,r] \ne \emptyset, [l',r'] \not \subseteq [l,r]$ 
%             allora si hanno diverse strategie in base alla percentuale di vettori in $[l',r']$ 
%             selezionati dalla query:
%             \begin{itemize}
%                 \item se in $[l',r']$ la query seleziona fino al $50\%$ dei vettori,
%                 si effettua una ricerca esaustiva delle $140$ migliori codifiche 
%                 più vicine alla codifica della query nella regione $[l,r] \cap [l',r']$.
%                 \item se in $[l',r']$ la query seleziona almeno $50\%$ dei vettori,
%                 si ricerca nell'HNSW con parametri $M=1180$ e $K=140$
%             \end{itemize} 
%         \end{itemize}
%         \item se la query seleziona almeno $2000000$ allora:
%         \begin{itemize}
%             \item se $[l',r'] \not \subseteq [l,r]$ allora si ignora il sottointervallo.
%             \item se $[l,r] = [l',r'] \lor  [l',r'] \subseteq [l,r]$ allora 
%             tutti i vettori del sottointervallo solo potenziali candidati e si 
%             hanno diverse strategie in base al numero di vettori selezionati nel 
%             dataset dalla query allora variano i parametri di ricerca:
%             \begin{itemize}
%                 \item se la query seleziona fino a $3000000$ allora ricerca 
%                 nell'HNSW di $[l',r']$ con $K=140$ e $M=1180$
%                 \item se la query seleziona da $3000000$ a $6000000$ allora ricerca 
%                 nell'HNSW di $[l',r']$ con $K=140$ e $M=780$
%                 \item se la query seleziona sopra $6000000$ allora ricerca 
%                 nell'HNSW di $[l',r']$ con $K=140$ e $M=680$
%             \end{itemize}
%             Infine dei $140$ vicini alla query estratti, si scelgono i migliori $100$ 
%             nello spazio originale.
%             \item se $[l',r'] \cap [l,r] \ne \emptyset, [l',r'] \not \subseteq [l,r]$ 
%             allora si hanno diverse strategie sempre in base al numero di vettori 
%             selezionati dalla query nel dataset:
%             \begin{itemize}
%                 \item se la query seleziona fino a $3000000$ allora ricerca 
%                 nell'HNSW di $[l',r']$ con $K=140$ e $M=780$
%                 \item se la query seleziona da $3000000$ a $6000000$ allora ricerca 
%                 nell'HNSW di $[l',r']$ con $K=140$ e $M=680$
%                 \item se la query seleziona sopra $6000000$ allora ricerca 
%                 nell'HNSW di $[l',r']$ con $K=140$ e $M=480$
%             \end{itemize}
%         \end{itemize}
%     \end{itemize} 
%     Quindi per ogni query che specifica un filtraggio sull'intervallo $[l,r]$ allora 
%     si scompone l'intervallo in tanti sottointervalli $[l',r']_i$ contigui dai quali 
%     si estraggono per ognuno $140$ vettori migliori secondo le codifiche, infine 
%     si aggregano e si prendono solo i $100$ migliori. 
% \end{itemize}




% - selezionata i vettori codificati in base alla tipologia di query
% - se < 500000 oppure query tipo 3 < 800000 allora fa bruteforce e ricerca le migliori $140$ codifiche a $112 B$
% più vicine a quella di query e poi seleziona le $100$ migliori rispetto ai valori 
% nativi
% - per tipo 1 > 500000 e tipo 3 > 800000  effetua una ricerca in HNSW $K= \lceil1800+(2500-1800)/(maxc_size - minc_size)dim_cat\rceil$
%  $K= \lceil1800+(2800-1800)/(maxc_size - minc_size)dim_cat\rceil$
%  Seleziona i migliori $150$ vettori che rispettano il filtro, infine estraggo i 
%  migliori $100$.

% - per tipo 0 oltre alla ricerca in brute force, ricerca nei grafi dei timestamp.
%  Più precisamente si ottengono $150$ candidati per ciascuna query dalla ricerca 
%  parallela nei grafi di ciascun timestamp. Più precisamente per ciascun grafo di 
%  timestamp si cercano $450$ vicini alla query, dai quali si estraggono solo i primi 
%  $150$ per ogni grafo e, infine, si aggregano i risultati in un pool 
%  di candidati per ogni query grande $150$ vettori preferendo sempre i più vicini.
%  (nota nella parte di bruteforce cerca un solo vicino, forse è un errore che hanno 
%  lasciato)

%  Avendo $10$ sottointervalli di timestamp si ottengono un totale di $150*10$ candidati 
%  e si aggregano scegliendo i migliori $100$.
 
% - tipo 2: se quelli selezionati rispetto al filtro sono > 500000 e > 800000 allora 
%  seleziona la strategia di ricerca in base alla copertura di $[l,r]$ sulle $10$ 
%  suddivisioni del timestamp, data una suddivisione $[l',r']$:
%     - se il numero di vettori selezionati in precedenza è < 2000000 e $[l,r]$ copre 
%     tutto $[l',r']$ allora si cerca direttamente nel grafo metrico dell'intervallo $[l',r']$ (ricerca FULL),
%     se [l,r] copre fino ad un massimo del $50\%$ della suddivisione [l',r'] allora 
%     ricerca in bruteforce in quell'intervallo. Altrimenti fai ricerca MEDIUM
%     - se il numero di vettori selezionati in precedenza è >= 2000000 fai la stessa 
%     cosa di sopra ma abbassando la threshold di bruteforce a  $20\%$

%   Effettua la ricerca della query rispetto alle strategie definite prima:
%     - bruteforce: i migliori 140 vettori nel sottointervallo [l',r'] intersecato [l,r]
%     e li mette nel pool di candidati.
%     - MEDIUM: se l'insieme dei vettori da confrontare è < 3000000 allora si estraggono 
%     dal sottointervallo [l',r'] i vicini $1180$ dalla query, se sono in totale <= 6000000 e >=  3000000
%     allora si estraggono dal sottointervallo [l',r'] i vicini $780$ dalla query, 
%     se sono in totale > 6000000 allora si estraggono dal sottointervallo [l',r'] i vicini $680$ dalla query.
%     e di questi se ne estraggono $150$
%     - FULL: se l'insieme dei vettori da confrontare è < 3000000 allora si estraggono 
%     dal sottointervallo [l',r'] i vicini $780$ dalla query, se sono in totale <= 6000000 e >=  3000000
%     allora si estraggono dal sottointervallo [l',r'] i vicini $630$ dalla query, 
%     se sono in totale > 6000000 allora si estraggono dal sottointervallo [l',r'] i vicini $480$ dalla query.
%     e di questi se ne estraggono $150$
  
%   In questo modo se $[l,r]$ include in totale $5$ sotto intervalli contigui [l',r']
%   allora si ottengono $150*5$ e infine si scelgono i migliori $100$.