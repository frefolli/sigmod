\chapter{Introduzione}

Il contest di programmazione ACM SIGMOD 2024 si è articolato nello sviluppo di una soluzione 
per una variante del KNN problem (con $K=100$) per un dataset $D$ di dimensione 
$10^7$ vettori e un insieme di query $Q$ di dimensione $4\cdot 10^6$. Ciascun 
vettore del dataset possiede dei metadati che specificano la sua categoria e il
suo timestamp. Dal momento che si hanno dei metadati associati ai vettori, si 
identificano anche $4$ tipologie di query, ognuna specifica dei filtri da applicare sui 
metadati del dataset:
\begin{itemize}
    \item Query tipo $0$: non si applicano filtri sui metadati.
    \item Query tipo $1$: si filtra il database rispetto alla categoria specificata della query, si considerano 
    solo i vettori che hanno la stessa categoria del vettore di query.
    \item Query tipo $2$: si filtra rispetto all'intervallo di timestamp specificato della query,
    si considerano solo i vettori che hanno il timestamp compreso nell'intervallo 
    specificato dalla query.
    \item query tipo $3$: si applicano entrambi i filtri della tipologia $1$ e $2$.
\end{itemize}

Le query, di conseguenza, avranno associato la tipologia, i metatati per filtrare 
in accordo con la tipolgia ed, infine, le $100$ componenti.

Il codice della risoluzione viene valutato in una macchina virtuale con $32$ Thread,
$64Gb$ di RAM e $64Gb$ di spazio. Si ha inoltre un tempo massimo di esecuzione 
di $20$ minuti.

Formalmente, il problema introdotto precedentemente può essere descritto nel 
seguente modo. Siano $D\subseteq \mathbb{R}^{100}$ e $Q\subseteq \mathbb{R}^{100}$
rispettivamente gli insiemi dei vettori di dataset e del queryset.

Siano $c:D\cup Q\to \mathbb{N}$ e $t:D\to \mathbb{R}_+$ le funzioni che restituiscono 
la categoria e il timestamp dei vettori del dataset e/o del queryset.
Sia $type: Q\to \{0, 1, 2, 3\}$ e $bound:Q \to \mathbb{R}^2_+$ le funzioni
che restituiscono rispettivamente la tipologia e l'intervallo di timestamp dei 
vettori del queryset.

Quindi la ricerca si effettua in base ai casi, $\forall q\in Q$:
\begin{itemize}
    \item $type(q) = 0$ (query senza filtraggio): si cerca $S_q$ ($S_q \subseteq D$ 
    e $|S_q| = 100$) tale che 
    $$\forall s \in S_q, d(s, q) \le d(x, q) \forall x \in D \setminus S_q$$  
    \item $type(q) = 1$ (query filtrando sulla categoria): si cerca $S_q$ ($S_q \subseteq \{x| x\in D, c(x) = c(q)\}$ 
    e $|S_q| = 100$) tale che 
    $$\forall s \in S_q, d(s, q) \le d(x, q) \forall x \in \{x| x\in D \setminus S_q, c(x) = c(q)\}$$  
    \item $type(q) = 2$ (query filtrando sull'intervallo di timestamp): si cerca 
    $S_q$ ($S_q \subseteq \{x| x\in D, t(x) \in timestamp(q)\}$ 
    e $|S_q| = 100$) tale che 
    $$\forall s \in S_q, d(s, q) \le d(x, q) \forall x \in \{x| x\in D \setminus S_q, t(x) \in bound(q)\}$$  
    \item $type(q) = 3$ (query filtrando sull'intervallo di timestamp e sulla categoria): si cerca 
    $S_q$ ($S_q \subseteq \{x| x\in D, t(x) \in bound(q),  c(x) = c(q)\}$ 
    e $|S_q| = 100$) tale che 
    $$\forall s \in S_q, d(s, q) \le d(x, q) \forall x \in \{x| x\in D \setminus S_q, t(x) \in bound(q), c(x) = c(q)\}$$  
\end{itemize}
Dove $d$ è la distanza euclidea applicata alle $100$ componenti dei vettori.
 
La soluzione finale $\mathcal{S}$ coinciderà con $ \mathcal{S} =\{S_q| \forall q \in Q\}$.

\section{Approcci tentati}
Un modo per risolvere questo problema è adottare l'approccio naive esaustivo (EN), ovvero 
confrontare ciascun vettore di query con tutti i vettori del database filtrato. 
A livello di complessità è insostenibile, infatti supponendo nel caso peggiore 
di avere solo query del tipo $0$, significa che si calcoleranno un totale di 
$\mathcal{O}(|Q| \cdot |D|)$ distanze, con l'aggiunta della complessità di creazione
della soluzione. La creazione della soluzione può essere trattata mediante l'ausilio 
di un max-heap, contenente le coppie vettore database e distanza vettore-query ($D\times \mathbb{R}_+$),
la dimensione può essere considerata fissa a $K(=100)$, questo significa che l'inserimento 
della singola coppia comporta una complessità $\mathcal{O}(K)$. Dal momento che 
si tenterà di inserire $\mathcal{O}(|D|)$ coppie allora la costruzione della 
soluzione per ogni singola query sarà $\mathcal{O}(|D|\cdot K)$, complessivamente per l'intero 
queryset si avrà $\mathcal{O}(|Q| \cdot |D|\cdot K)$. Dal momento che $K$ è costante 
a $100$ allora si può approssimare la complessità di costruzione della soluzione 
a  $\mathcal{O}(|Q| \cdot |D|)$. Complessità che risultano essere proibitive quando 
si lavora su $|D|= 10^7$ e $|Q|= 4\cdot 10^6$, infatti supponendo di impiegare $1ms$ 
a calcolare una singola distanza allora significa che per effettuare solo tutti i 
confronti si impiegherà $0,001 \cdot |D| \cdot |Q| = 4\cdot 10^{10}$
secondi che vuol dire circa $1268$ giorni, anche parallelizzando al massimo 
il tutto si impiegherà circa $1268/32 = 39$ giorni. 

EN non può essere adottata come soluzione finale, ma è stata utile per poter 
stimare la Recall dei metodi sviluppati in seguito. Infatti, dal momento che i 
creatori della challenge hanno rilasciato differenti versioni del dataset ($10^4, 10^6, 10^7$) e del 
queryset ($10^2, 10^5, 10^6$), è stato possibile confrontare le soluzioni di EN
e dei metodi sviluppati sui dataset con una grandezza minore. 

Visto che EN è proibitivo, è stata effettuata una ricerca sulla letteratura 
relativa all'argomento. Da questa ricerca sono stati estratti diversi approci, 
un primo approccio si basa sull'uso di alberi metrici come \textbf{KD-tree} \cite{kd_tree}, \textbf{MVP-tree}
\cite{mvp_tree1} \cite{mvp_tree2}, \textbf{VP-tree} \cite{vp_tree} e \textbf{Ball-tree} \cite{ball_tree}
che funzionano bene per vettori che hanno una dimensione ridotta, ma esplodono 
di complessità quando vengono eseguiti su dimensioni elevate come nell'attuale 
caso in esame. Perciò è stato pensato di applicare i metodi dopo aver applicato 
un algoritmo di riduzione di dimensionalità con l'obiettivo di ridurre 
i tempi di esecuzione.

Sono stati testati gli algoritmi di riduzione come \textbf{Principal Component Analysis} (PCA) \cite{pca}
e \textbf{Random Projection} (RP) \cite{rp}. PCA è stato immediatamente scartato dal momento che 
calcolando la matrice di correlazione sul dataset $D$ si è ottenuta un'assenza 
di correlazione tra le componenti che preannuncia una spiegazione di variabilità 
di dati uniforme tra tutti gli autovettori della matrice di correlazione, comportando 
una mancanza di componenti con maggior information gain. Perciò è stato provato 
un approccio alternativo con RP che ha permesso di ridurre la 
dimensionalità a $90$ componenti per preservare la recall sopra il $90\%$, ma 
ciò non ha permesso di rendere utilizzabili gli alberi metrici a causa gli eccessivi 
tempi di costruzione e ricerca.

Successivamente è stato tentato l'approccio dei grafi metrici con \textbf{Hierarchical 
Navigable Small World Graphs} (HNSW) \cite{hnsw}. A livello teorico permette di ridurre i tempi
di ricerca (infatti la complessità è $\mathcal{O}(log |D|)$), il problema è 
che la costruzione della struttura è risultata troppo pesante e non è stato trovato 
un modo per migliorare i tempi. Il problema principale della costruzione è 
il fatto che alla fine si va a calcolare un totale di $\mathcal{O}(|D|\cdot |D|)$
distanze che a livello di tempo è risultato proibitivo, sopprattuto perché 
iterativamente si deve aggiungere un vettore alla volta, rendendo complesso parallelizzare 
la costruzione.

Successivamente è stato tentato l'approccio della quantizzazione dei vettori con 
la \textbf{Product Quantization} (PQ) \cite{pq} e gli \textbf{Inverted File Index} (IVF)\cite{pq}. Il problema di questi 
due approcci è che richiedono troppo tempo per costruire l'indice dal momento che 
entrambi i metodi nascondono diverse esecuzioni di Kmeans parzialmente parallelizzabili
e per questo non è stato possibile utilizzarli per via dei tempi di costruzione 
proibitivi. Per rendere questi approcci utilizzabili bisognava ridurre notevolmente 
il numero di centroidi comportando un aumento dell'errore di quantizzazioen dei 
vettori e peggiorando, di conseguenza, la Recall.

Infine è stata trovato l'approccio finale basato su $LSH$ che ha permesso, attraverso 
una fase di fine-tuning dei parametri, di raggiungere un compromesso tra tempi di 
esecuzione e Recall discreti.
